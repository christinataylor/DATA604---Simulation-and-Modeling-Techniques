---
title: 'Homework #4'
author: "J. Hamski"
date: "10/13/2016"
output: pdf_document
---
```{r, cache=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(grid)
library(gridExtra)
library(dplyr)
```

```{r}
set.seed(1272)
```
#Variance reduction procedures for Monte Carlo methods  
## Preliminary Setup  
The multiplier decreases as D increases. 
```{r, cache=FALSE}
first.multiplier <- function(D){
  return(1/((2*pi)^(D/2)))
}
```

```{r, cache=FALSE}
D.seq <- seq(1, from=1, to=10)

first.multiplier.list <- sapply(D.seq, FUN=first.multiplier)

plot(first.multiplier.list,  type = "l")
```

Testing the exponent function. 
```{r}
exponent.function <- function(x){
  return((-1/2)*(t(x)%*%x))
}
n <- 10
D <- 2
x <- matrix(runif(D*n, min = -5, max = 5), ncol=D)

exponent.function(x)
```

These don't really need to be broken out into different functions, but I found it helpful to try different inputs and see how the components of the function behave. 

##a) Crude Monte Carlo  
First, create a function to evaluate c(x) at N samples in D dimensions (i.e., a DxN matrix).  
```{r, cache=FALSE}
cost.function.crude <- function(x, D){
  first.multiplier.sim <- first.multiplier(D)
  exponent.sim <- exponent.function(x)
  return(first.multiplier.sim * exp(exponent.sim))
}
```

```{r, cache=FALSE}
n <- 1
D <- 2
x <- matrix(runif(D*n, min = -5, max = 5), nrow=n, ncol=D)
```

```{r, cache=FALSE}
cost.function.crude.apply <- function(x, D){
  return(mean(apply(x, 1, FUN = cost.function.crude, D=D)))
}
cost.function.crude.apply(x, D)
```

This appears to work (the mean is always close to 1/10). Now to move on to the crude Monte Carlo simulation of the cost function for n sizes from 1000 to 10,000 by increments of 1000.

```{r, cache=FALSE}
n.increments <- seq(from = 1000, to = 10000, by = 1000)

generate.x.list <- function(n, D){
  x <- matrix(runif(D*n, min = -5, max = 5), nrow=n, ncol=D)
  return(x)
}

x.list <- lapply(n.increments, FUN = generate.x.list, D=D)
```

For D = 1:
```{r, cache=FALSE}
D.1 <- 1
crude.cost.sim.1 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list, D=D.1), FUN = cost.function.crude.apply, D=D.1))
```

```{r, cache=FALSE}
crude.cost.sim.1.means <- apply(crude.cost.sim.1, 1, FUN = mean)
crude.cost.sim.1.sd <- apply(crude.cost.sim.1, 1, FUN = sd)
n <- n.increments
crude.cost.sim.1.results <- cbind(n, crude.cost.sim.1.means, crude.cost.sim.1.sd) %>% as.data.frame()
colnames(crude.cost.sim.1.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=FALSE}
limits <- aes(ymax = Cost.Means + Cost.Standard.Deviation, ymin = Cost.Means - Cost.Standard.Deviation)
ggplot(crude.cost.sim.1.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits)
```

```{r, cache=FALSE}
crude.cost.sim.1.results <- crude.cost.sim.1.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r}
kable(crude.cost.sim.1.results)
```


Now for D = 2: 

```{r, cache=FALSE}
D.2 = 2
crude.cost.sim.2 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list, D = D.2), FUN = cost.function.crude.apply, D = D.2))

(target.D.2 <- (1/10)^2)
```

```{r, cache=FALSE}
crude.cost.sim.2.means <- apply(crude.cost.sim.2, 1, FUN = mean)
crude.cost.sim.2.sd <- apply(crude.cost.sim.2, 1, FUN = sd)

crude.cost.sim.2.results <- cbind(n, crude.cost.sim.2.means, crude.cost.sim.2.sd) %>% as.data.frame()
colnames(crude.cost.sim.2.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=FALSE}
limits.2 <- aes(ymax = Cost.Means + Cost.Standard.Deviation, ymin = Cost.Means - Cost.Standard.Deviation)
ggplot(crude.cost.sim.2.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits.2)
```

```{r, cache=FALSE}
crude.cost.sim.2.results <- crude.cost.sim.2.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r}
kable(crude.cost.sim.2.results)
```
  
##b) Quasi-Random Numbers   

Here, I use the package 'randtoolbox' to generate a Sobol sequence. 
```{r, message=FALSE, warning=FALSE}
library(randtoolbox)
```

```{r, fig.width=6, fig.height=3}
n = 100
sobol.xy <- sobol(n, dim = 2) %>% as.data.frame()
sobol <- qplot(sobol.xy$V1, sobol.xy$V2, main="Sobol")

unif.xy <- cbind(runif(n), runif(n)) %>% as.data.frame()
rand <- qplot(unif.xy$V1, unif.xy$V2, main = "Random")

grid.arrange(sobol, rand, nrow=1)
```

The random numbers (uniform distribution) form clusters where there are several random variables with close values and "dead space" where there are no random variables with the values. 

The difference between the two is their discrepency. The Sobol numbers are low discrepency but are "quasi-random" - the enforcement of even distribution means that the values are not i.i.d. and therefore not truly random. The uniform distribution random numbers have the potential to be high discrepancy. 

For D = 1:
```{r, cache=FALSE}
generate.x.list.Sobol <- function(n, D){
  sobol.seq <- (sobol(n, dim = D) * 10) - 5 
  x <- as.matrix(sobol.seq)
  return(x)
}
```

```{r, cache=FALSE}
D.1 <- 1
sobol.cost.sim.1 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.Sobol, D=D.1), FUN = cost.function.crude.apply, D=D.1))
```

```{r, echo=FALSE}
sobol.cost.sim.1.means <- apply(sobol.cost.sim.1, 1, FUN = mean)
sobol.cost.sim.1.sd <- apply(sobol.cost.sim.1, 1, FUN = sd)
n <- n.increments
sobol.cost.sim.1.results <- cbind(n, sobol.cost.sim.1.means, sobol.cost.sim.1.sd) %>% as.data.frame()
colnames(sobol.cost.sim.1.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=FALSE, echo=FALSE}
ggplot(sobol.cost.sim.1.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits)
```

```{r, cache=FALSE, echo=FALSE}
sobol.cost.sim.1.results <- sobol.cost.sim.1.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r, echo=FALSE}
kable(sobol.cost.sim.1.results)
```

For D = 2: 

```{r, cache=FALSE, echo=FALSE}
D.2 = 2
sobol.cost.sim.2 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.Sobol, D = D.2), FUN = cost.function.crude.apply, D = D.2))
```

```{r, cache=FALSE, echo=FALSE}
sobol.cost.sim.2.means <- apply(sobol.cost.sim.2, 1, FUN = mean)
sobol.cost.sim.2.sd <- apply(sobol.cost.sim.2, 1, FUN = sd)

sobol.cost.sim.2.results <- cbind(n, sobol.cost.sim.2.means, sobol.cost.sim.2.sd) %>% as.data.frame()
colnames(sobol.cost.sim.2.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=FALSE, echo=FALSE}
ggplot(sobol.cost.sim.2.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits.2)
```

```{r, cache=FALSE, echo=FALSE}
sobol.cost.sim.2.results <- sobol.cost.sim.2.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r, cache=FALSE, echo=FALSE}
kable(sobol.cost.sim.2.results)
```

##c) Antithetic Variates
```{r, cache=FALSE}

generate.x.list.antithetic <- function(n, D){
  x <- matrix(runif(D*n, min = 0, max = 1), nrow=n, ncol=D)

  for(i in 1:nrow(x)){
    ifelse(i %% 2 == 0, x[i,] <- 1 - x[i-1,], next)
  }
  x <- (x * 10) - 5
  return(x)
}
```

```{r}
generate.x.list.antithetic(4, 3)
```

```{r, cache=FALSE}
D.1 <- 1
antithetic.cost.sim.1 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.antithetic, D=D.1), FUN = cost.function.crude.apply, D=D.1))
```

```{r, cache=FALSE, echo=FALSE}
antithetic.cost.sim.1.means <- apply(antithetic.cost.sim.1, 1, FUN = mean)
antithetic.cost.sim.1.sd <- apply(antithetic.cost.sim.1, 1, FUN = sd)
n <- n.increments
antithetic.cost.sim.1.results <- cbind(n, antithetic.cost.sim.1.means, antithetic.cost.sim.1.sd) %>% as.data.frame()
colnames(antithetic.cost.sim.1.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=FALSE, echo=FALSE}
limits <- aes(ymax = Cost.Means + Cost.Standard.Deviation, ymin = Cost.Means - Cost.Standard.Deviation)
ggplot(antithetic.cost.sim.1.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits)
```

```{r, cache=FALSE, echo=FALSE}
antithetic.cost.sim.1.results <- antithetic.cost.sim.1.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r, echo=FALSE}
kable(antithetic.cost.sim.1.results)
```

For D = 2: 

```{r, cache=FALSE}
D.2 = 2
antithetic.cost.sim.2 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.antithetic, D = D.2), FUN = cost.function.crude.apply, D = D.2))
```

```{r, cache=FALSE, echo=FALSE}
antithetic.cost.sim.2.means <- apply(antithetic.cost.sim.2, 1, FUN = mean)
antithetic.cost.sim.2.sd <- apply(antithetic.cost.sim.2, 1, FUN = sd)

antithetic.cost.sim.2.results <- cbind(n, antithetic.cost.sim.2.means, antithetic.cost.sim.2.sd) %>% as.data.frame()
colnames(antithetic.cost.sim.2.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=FALSE, echo=FALSE}
limits.2 <- aes(ymax = Cost.Means + Cost.Standard.Deviation, ymin = Cost.Means - Cost.Standard.Deviation)
ggplot(antithetic.cost.sim.2.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits.2)
```

```{r, cache=FALSE, echo=FALSE}
antithetic.cost.sim.2.results <- antithetic.cost.sim.2.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r, echo=FALSE}
kable(antithetic.cost.sim.2.results)
```

##d) Latin Hypercube Sampling
```{r}
library(lhs)
```

For D = 1:
```{r, cache=FALSE}
generate.x.list.latin <- function(n, D){
  x <- as.matrix((randomLHS(n,D) * 10) - 5 )
  return(x)
}
```

```{r, cache=FALSE, echo=FALSE}
D.1 <- 1
latin.cost.sim.1 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.latin, D=D.1), FUN = cost.function.crude.apply, D=D.1))
```

```{r, cache=FALSE, echo=FALSE}
latin.cost.sim.1.means <- apply(latin.cost.sim.1, 1, FUN = mean)
latin.cost.sim.1.sd <- apply(latin.cost.sim.1, 1, FUN = sd)
n <- n.increments
latin.cost.sim.1.results <- cbind(n, latin.cost.sim.1.means, latin.cost.sim.1.sd) %>% as.data.frame()
colnames(latin.cost.sim.1.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=FALSE, echo=FALSE}
ggplot(latin.cost.sim.1.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits)
```

```{r, cache=FALSE, echo=FALSE}
latin.cost.sim.1.results <- latin.cost.sim.1.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r, echo=FALSE}
kable(latin.cost.sim.1.results)
```

For D = 2: 

```{r, cache=TRUE}
D.2 = 2
latin.cost.sim.2 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.latin, D = D.2), FUN = cost.function.crude.apply, D = D.2))
```

```{r, cache=FALSE, echo=FALSE}
latin.cost.sim.2.means <- apply(sobol.cost.sim.2, 1, FUN = mean)
latin.cost.sim.2.sd <- apply(sobol.cost.sim.2, 1, FUN = sd)

latin.cost.sim.2.results <- cbind(n, latin.cost.sim.2.means, latin.cost.sim.2.sd) %>% as.data.frame()
colnames(latin.cost.sim.2.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=FALSE, echo=FALSE}
ggplot(latin.cost.sim.2.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits.2)
```

```{r, cache=FALSE, echo=FALSE}
latin.cost.sim.2.results <- latin.cost.sim.2.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r, cache=FALSE, echo=FALSE}
kable(latin.cost.sim.2.results)
```
##e) Importance Sampling



##f) Summary
```{r}
summary.cost <- rbind(crude.cost.sim.1.results[c(1,4),], crude.cost.sim.2.results[c(1,4),],
                      sobol.cost.sim.1.results[c(1,4),], sobol.cost.sim.2.results[c(1,4),],
                      antithetic.cost.sim.1.results[c(1,4),], antithetic.cost.sim.2.results[c(1,4),],
                      latin.cost.sim.1.results[c(1,4),], latin.cost.sim.2.results[c(1,4),])
dimensions <- rep(c(1,1,2,2), 4)
method <- c(rep("crude",4), rep("sobol", 4), rep("antithetic", 4), rep("latin",4))

summary.cost <- cbind(method, dimensions, summary.cost)
summary.cost$upper <- summary.cost$Cost.Means + summary.cost$Cost.Standard.Deviation
summary.cost$lower <- summary.cost$Cost.Means - summary.cost$Cost.Standard.Deviation

kable(summary.cost)
```

```{r}
ggplot(summary.cost %>% filter(dimensions==1), aes(factor(method), Cost.Means, color = factor(n))) + 
  geom_point() + ggtitle("One Dimension")
```


```{r}
ggplot(summary.cost %>% filter(dimensions==2), aes(factor(method), Cost.Means, color = factor(n))) + 
  geom_point() + ggtitle("Two Dimensions")
```

```{r}
ggplot(summary.cost %>% filter(dimensions==1), aes(factor(method), Cost.Standard.Deviation, color = factor(n))) + 
  geom_bar(stat = "identity") + ggtitle("One Dimension")
```

```{r}
ggplot(summary.cost %>% filter(dimensions==2), aes(factor(method), Cost.Standard.Deviation, color = factor(n))) + 
  geom_bar(stat = "identity") + ggtitle("One Dimension")
```
Variance was eliminated with the Latin Hypercube sampling and Sobol sampling due to the repeating nature of the distributions. Antithetic sampling increased variance compared to the crude Monte Carlo method.  

#6.3
*Plot the power curves for the t-test in Example 6.9 for sample sizes 10, 20, 30, 40, and 50, but omit the standard error bars. Plot the curves on the same craph, each in a different color or line type, and include a legend. Comment on the relation between power and sample size.*

```{r}
empirical.power <- function(n){
  m = 1000
  mu0 = 500
  sigma = 100
  mu <- c(seq(450, 650, 10))
  M <- length(mu)
  power <- numeric(M)
  
  for (i in 1:M) {
    mu1 <- mu[i]
    pvalues <- replicate(m, expr = {
      x <- rnorm(n, mean = mu1, sd = sigma)
      ttest <- t.test(x, alternative = "greater", 
                      mu = mu0)
      ttest$p.value})
    power[i] <- mean(pvalues <= 0.05)
  }
  return(power)
}
```

```{r}
n.set <- seq(from = 10, to = 50, by = 10)

power.sim <- lapply(n.set, FUN = empirical.power)
```

```{r, warning=FALSE, message=FALSE}
theta <- seq(450, 650, 10)

tidy.results <- NULL

for (i in 1:length(n.set)){
  
  results.set <- cbind(rep(i*10, times = length(n.set)), 
                       theta, unlist(power.sim[i]))
  tidy.results <- rbind(results.set, tidy.results)
}

tidy.results.6.3 <- as.data.frame(tidy.results) 
colnames(tidy.results.6.3) <- c("Sample.Size", "Theta", "Power")
tidy.results.6.3$Sample.Size <- as.factor(tidy.results.6.3$Sample.Size)
```

```{r}
ggplot(tidy.results.6.3) + 
  geom_line(aes(x = Theta, y = Power, group = Sample.Size, color = Sample.Size))
```


#6.4
*Suppose that $X_1,...,X_n$ are a random sample from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter u. Use a Monte Carlo method to obtain an empirical estimate of the the confidence level.*

```{r}
set.seed(4388)
n = 20
alpha = 0.5
reps = 1000

mlg <- runif(1, min = 0, max = 1)
sdlg <- runif(1, min = 0, max = 2)

calcCI <- function(n, alpha){
  x <- rlnorm(n, mlg, sdlg)
  return((n-1) * var(x) / qchisq(alpha, df = n-1))
}

UCL <- replicate(reps, calcCI(n, alpha))

sum(UCL > sdlg^2)
mean(UCL > sdlg^2)
```

Because the lognormal distribution is very close to the normal distribution, the confidence level is quite high. 

#7.1
*Compute the jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.*

```{r}
data(law, package = 'bootstrap')
law <- law
```

```{r}
n <- nrow(law)
lsat <- law$LSAT
gpa <- law$GPA

(theta.hat <-cor(lsat, gpa))
```

```{r}
theta.jack <- numeric(n)

for (i in 1:n){
  theta.jack[i] <- cor(lsat[-i], gpa[-i])
}

(bais <- (n-1) * (mean(theta.jack) - theta.hat))
```

```{r}
(se <- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2)))
```


#7.4
*Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of ac equiment. Assme that the times between failures follow an exponential model. Obtain the MLE of the hazard rate and use bootstrap to estimate the bias and standard error of the estimate.* 

```{r}
data(aircondit, package = 'boot')
aircondit <- aircondit
```

```{r}

```

