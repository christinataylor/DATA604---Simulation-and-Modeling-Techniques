---
title: 'Homework #4'
author: "J. Hamski"
date: "10/13/2016"
output: pdf_document
---
```{r, cache=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(grid)
library(gridExtra)
```

```{r}
set.seed(1272)
```
#Variance reduction procedures for Monte Carlo methods  
## Preliminary Setup  
The multiplier decreases as D increases. 
```{r, cache=TRUE}
first.multiplier <- function(D){
  return(1/((2*pi)^(D/2)))
}
```

```{r, cache=TRUE}
D.seq <- seq(1, from=1, to=10)

first.multiplier.list <- sapply(D.seq, FUN=first.multiplier)

plot(first.multiplier.list,  type = "l")
```

Testing the exponent function. 
```{r}
exponent.function <- function(x){
  return((-1/2)*(t(x)%*%x))
}
n <- 10
D <- 2
x <- matrix(runif(D*n, min = -5, max = 5), ncol=D)

exponent.function(x)
```

These don't really need to be broken out into different functions, but I found it helpful to try different inputs and see how the components of the function behave. 

##a) Crude Monte Carlo  
First, create a function to evaluate c(x) at N samples in D dimensions (i.e., a DxN matrix).  
```{r, cache=TRUE}
cost.function.crude <- function(x, D){
  first.multiplier.sim <- first.multiplier(D)
  exponent.sim <- exponent.function(x)
  return(first.multiplier.sim * exp(exponent.sim))
}
```

```{r, cache=TRUE}
n <- 1
D <- 2
x <- matrix(runif(D*n, min = -5, max = 5), nrow=n, ncol=D)
```

```{r, cache=TRUE}
cost.function.crude.apply <- function(x, D){
  return(mean(apply(x, 1, FUN = cost.function.crude, D=D)))
}
cost.function.crude.apply(x, D)
```

This appears to work (the mean is always close to 1/10). Now to move on to the crude Monte Carlo simulation of the cost function for n sizes from 1000 to 10,000 by increments of 1000.

```{r, cache=TRUE}
n.increments <- seq(from = 1000, to = 10000, by = 1000)

generate.x.list <- function(n, D){
  x <- matrix(runif(D*n, min = -5, max = 5), nrow=n, ncol=D)
  return(x)
}

x.list <- lapply(n.increments, FUN = generate.x.list, D=D)
```

For D = 1:
```{r, cache=TRUE}
D.1 <- 1
crude.cost.sim.1 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list, D=D.1), FUN = cost.function.crude.apply, D=D.1))
```

```{r, cache=TRUE}
crude.cost.sim.1.means <- apply(crude.cost.sim.1, 1, FUN = mean)
crude.cost.sim.1.sd <- apply(crude.cost.sim.1, 1, FUN = sd)
n <- n.increments
crude.cost.sim.1.results <- cbind(n, crude.cost.sim.1.means, crude.cost.sim.1.sd) %>% as.data.frame()
colnames(crude.cost.sim.1.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=TRUE}
limits <- aes(ymax = Cost.Means + Cost.Standard.Deviation, ymin = Cost.Means - Cost.Standard.Deviation)
ggplot(crude.cost.sim.1.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits)
```

```{r, cache=TRUE}
crude.cost.sim.1.results <- crude.cost.sim.1.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r}
kable(crude.cost.sim.1.results)
```


Now for D = 2: 

```{r, cache=TRUE}
D.2 = 2
crude.cost.sim.2 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list, D = D.2), FUN = cost.function.crude.apply, D = D.2))

(target.D.2 <- (1/10)^2)
```

```{r, cache=TRUE}
crude.cost.sim.2.means <- apply(crude.cost.sim.2, 1, FUN = mean)
crude.cost.sim.2.sd <- apply(crude.cost.sim.2, 1, FUN = sd)

crude.cost.sim.2.results <- cbind(n, crude.cost.sim.2.means, crude.cost.sim.2.sd) %>% as.data.frame()
colnames(crude.cost.sim.2.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=TRUE}
limits.2 <- aes(ymax = Cost.Means + Cost.Standard.Deviation, ymin = Cost.Means - Cost.Standard.Deviation)
ggplot(crude.cost.sim.2.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits.2)
```

```{r, cache=TRUE}
crude.cost.sim.2.results <- crude.cost.sim.2.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r}
kable(crude.cost.sim.2.results)
```
  
##b) Quasi-Random Numbers   

Here, I use the package 'randtoolbox' to generate a Sobol sequence. 
```{r, message=FALSE, warning=FALSE}
library(randtoolbox)
```

```{r, fig.width=6, fig.height=3}
n = 100
sobol.xy <- sobol(n, dim = 2) %>% as.data.frame()
sobol <- qplot(sobol.xy$V1, sobol.xy$V2, main="Sobol")

unif.xy <- cbind(runif(n), runif(n)) %>% as.data.frame()
rand <- qplot(unif.xy$V1, unif.xy$V2, main = "Random")

grid.arrange(sobol, rand, nrow=1)
```

The random numbers (uniform distribution) form clusters where there are several random variables with close values and "dead space" where there are no random variables with the values. 

The difference between the two is their discrepency. The Sobol numbers are low discrepency but are "quasi-random" - the enforcement of even distribution means that the values are not i.i.d. and therefore not truly random. The uniform distribution random numbers have the potential to be high discrepancy. 

For D = 1:
```{r, cache=TRUE}
generate.x.list.Sobol <- function(n, D){
  sobol.seq <- (sobol(n, dim = D) * 10) - 5 
  x <- as.matrix(sobol.seq)
  return(x)
}
```

```{r, cache=TRUE}
D.1 <- 1
sobol.cost.sim.1 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.Sobol, D=D.1), FUN = cost.function.crude.apply, D=D.1))
```

```{r, cache=TRUE}
sobol.cost.sim.1.means <- apply(sobol.cost.sim.1, 1, FUN = mean)
sobol.cost.sim.1.sd <- apply(sobol.cost.sim.1, 1, FUN = sd)
n <- n.increments
sobol.cost.sim.1.results <- cbind(n, sobol.cost.sim.1.means, sobol.cost.sim.1.sd) %>% as.data.frame()
colnames(sobol.cost.sim.1.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=TRUE}
ggplot(sobol.cost.sim.1.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits)
```

```{r, cache=TRUE}
sobol.cost.sim.1.results <- sobol.cost.sim.1.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r}
kable(sobol.cost.sim.1.results)
```

For D = 2: 

```{r, cache=TRUE}
D.2 = 2
sobol.cost.sim.2 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.Sobol, D = D.2), FUN = cost.function.crude.apply, D = D.2))
```

```{r, cache=TRUE}
sobol.cost.sim.2.means <- apply(sobol.cost.sim.2, 1, FUN = mean)
sobol.cost.sim.2.sd <- apply(sobol.cost.sim.2, 1, FUN = sd)

sobol.cost.sim.2.results <- cbind(n, sobol.cost.sim.2.means, sobol.cost.sim.2.sd) %>% as.data.frame()
colnames(sobol.cost.sim.2.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=TRUE}
ggplot(sobol.cost.sim.2.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits.2)
```

```{r, cache=TRUE}
sobol.cost.sim.2.results <- sobol.cost.sim.2.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r, cache=TRUE}
kable(sobol.cost.sim.2.results)
```

##c) Antithetic Variates
```{r, cache=TRUE}

generate.x.list.antithetic <- function(n, D){
  x <- matrix(runif(D*n, min = 0, max = 1), nrow=n, ncol=D)

  for(i in 1:nrow(x)){
    ifelse(i %% 2 == 0, x[i,] <- 1 - x[i-1,], next)
  }
  x <- (x * 10) - 5
  return(x)
}
```

```{r}
generate.x.list.antithetic(4, 3)
```

```{r, cache=TRUE}
D.1 <- 1
antithetic.cost.sim.1 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.antithetic, D=D.1), FUN = cost.function.crude.apply, D=D.1))
```

```{r, cache=TRUE}
antithetic.cost.sim.1.means <- apply(antithetic.cost.sim.1, 1, FUN = mean)
antithetic.cost.sim.1.sd <- apply(antithetic.cost.sim.1, 1, FUN = sd)
n <- n.increments
antithetic.cost.sim.1.results <- cbind(n, antithetic.cost.sim.1.means, antithetic.cost.sim.1.sd) %>% as.data.frame()
colnames(antithetic.cost.sim.1.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=TRUE}
limits <- aes(ymax = Cost.Means + Cost.Standard.Deviation, ymin = Cost.Means - Cost.Standard.Deviation)
ggplot(antithetic.cost.sim.1.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits)
```

```{r, cache=TRUE}
antithetic.cost.sim.1.results <- antithetic.cost.sim.1.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r}
kable(antithetic.cost.sim.1.results)
```

For D = 2: 

```{r, cache=TRUE}
D.2 = 2
antithetic.cost.sim.2 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.antithetic, D = D.2), FUN = cost.function.crude.apply, D = D.2))
```

```{r, cache=TRUE}
antithetic.cost.sim.2.means <- apply(antithetic.cost.sim.2, 1, FUN = mean)
antithetic.cost.sim.2.sd <- apply(antithetic.cost.sim.2, 1, FUN = sd)

antithetic.cost.sim.2.results <- cbind(n, antithetic.cost.sim.2.means, antithetic.cost.sim.2.sd) %>% as.data.frame()
colnames(antithetic.cost.sim.2.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=TRUE}
limits.2 <- aes(ymax = Cost.Means + Cost.Standard.Deviation, ymin = Cost.Means - Cost.Standard.Deviation)
ggplot(antithetic.cost.sim.2.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits.2)
```

```{r, cache=TRUE}
antithetic.cost.sim.2.results <- antithetic.cost.sim.2.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r}
kable(antithetic.cost.sim.2.results)
```

##d) Latin Hypercube Sampling
```{r}
library(lhs)
```

For D = 1:
```{r, cache=TRUE}
generate.x.list.latin <- function(n, D){
  x <- as.matrix((randomLHS(n,D) * 10) - 5 )
  return(x)
}
```

```{r, cache=TRUE}
D.1 <- 1
latin.cost.sim.1 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.latin, D=D.1), FUN = cost.function.crude.apply, D=D.1))
```

```{r, cache=TRUE}
latin.cost.sim.1.means <- apply(latin.cost.sim.1, 1, FUN = mean)
latin.cost.sim.1.sd <- apply(latin.cost.sim.1, 1, FUN = sd)
n <- n.increments
latin.cost.sim.1.results <- cbind(n, latin.cost.sim.1.means, latin.cost.sim.1.sd) %>% as.data.frame()
colnames(latin.cost.sim.1.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=TRUE}
ggplot(latin.cost.sim.1.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits)
```

```{r, cache=FALSE}
latin.cost.sim.1.results <- latin.cost.sim.1.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r}
kable(latin.cost.sim.1.results)
```

For D = 2: 

```{r, cache=TRUE}
D.2 = 2
latin.cost.sim.2 <- replicate(100, sapply(lapply(n.increments, FUN = generate.x.list.latin, D = D.2), FUN = cost.function.crude.apply, D = D.2))
```

```{r, cache=TRUE}
latin.cost.sim.2.means <- apply(sobol.cost.sim.2, 1, FUN = mean)
latin.cost.sim.2.sd <- apply(sobol.cost.sim.2, 1, FUN = sd)

latin.cost.sim.2.results <- cbind(n, latin.cost.sim.2.means, latin.cost.sim.2.sd) %>% as.data.frame()
colnames(latin.cost.sim.2.results) <- c("n", "Cost.Means", "Cost.Standard.Deviation")
```

```{r, cache=TRUE}
ggplot(latin.cost.sim.2.results, aes(x=n, y=Cost.Means)) + geom_line() + geom_errorbar(limits.2)
```

```{r, cache=TRUE}
latin.cost.sim.2.results <- latin.cost.sim.2.results %>%
  mutate(Cost.Coeff.Varation = Cost.Standard.Deviation / Cost.Means)
```

```{r, cache=TRUE}
kable(latin.cost.sim.2.results)
```
##e) Importance Sampling



##f) Summary


#6.3
*Plot the power curves for the t-test in Example 6.9 for sample sizes 10, 20, 30, 40, and 50, but omit the standard error bars. Plot the curves on the same craph, each in a different color or line type, and include a legend. Comment on the relation between power and sample size.*

```{r}
empirical.power <- function(n){
  m = 1000
  mu0 = 500
  sigma = 100
  mu <- c(seq(450, 650, 10))
  M <- length(mu)
  power <- numeric(M)
  
  for (i in 1:M) {
    mu1 <- mu[i]
    pvalues <- replicate(m, expr = {
      x <- rnorm(n, mean = mu1, sd = sigma)
      ttest <- t.test(x, alternative = "greater", 
                      mu = mu0)
      ttest$p.value})
    power[i] <- mean(pvalues <= 0.05)
  }
  return(power)
}
```

```{r}
n.set <- seq(from = 10, to = 50, by = 10)

power.sim <- lapply(n.set, FUN = empirical.power)
```

```{r, warning=FALSE, message=FALSE}
theta <- seq(450, 650, 10)

tidy.results <- NULL

for(i in 1:length(n.set)){
  
  results.set <- cbind(rep(i*10, times = length(n.set)), theta,
                           unlist(power.sim[i]))
  tidy.results <- rbind(results.set, tidy.results)
}

tidy.results <- as.data.frame(tidy.results) 
colnames(tidy.results) <- c("Sample.Size", "Theta", "Power")
tidy.results$Sample.Size <- as.factor(tidy.results$Sample.Size)
```

```{r}
ggplot(tidy.results) + geom_line(aes(x = Theta, y = Power, group = Sample.Size, color = Sample.Size))
```


#6.4
*Suppose that $X_1,...,X_n$ are a random sample from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter u. Use a Monte Carlo method to obtain an empirical estimate of the the confidence level.*



#7.1

#7.4

